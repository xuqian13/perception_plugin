"""
è¯­è¨€é£æ ¼æ„ŸçŸ¥æ¨¡å—
åˆ†æç”¨æˆ·çš„è¯­è¨€é£æ ¼ã€å¸¸ç”¨è¯æ±‡ã€è¡¨è¾¾ä¹ æƒ¯ç­‰
"""

import time
import re
from typing import Dict, Optional, Any, List
from dataclasses import dataclass, asdict
from collections import Counter, defaultdict
from src.common.logger import get_logger

logger = get_logger("language_style_perception")


@dataclass
class LanguageStyle:
    """è¯­è¨€é£æ ¼æ•°æ®ç±»"""

    user_id: str = ""
    user_nickname: str = ""

    # é£æ ¼åˆ†ç±»
    formality: str = "neutral"  # "formal"(æ­£å¼) | "casual"(éšæ„) | "neutral"(ä¸­æ€§)
    tone: str = "neutral"  # "serious"(ä¸¥è‚ƒ) | "humorous"(å¹½é»˜) | "friendly"(å‹å¥½) | "neutral"(ä¸­æ€§)
    politeness: str = "neutral"  # "polite"(ç¤¼è²Œ) | "casual"(éšæ„) | "neutral"(ä¸­æ€§)

    # è¯­è¨€ç‰¹å¾
    avg_message_length: float = 0.0  # å¹³å‡æ¶ˆæ¯é•¿åº¦
    vocabulary_richness: float = 0.0  # è¯æ±‡ä¸°å¯Œåº¦ 0.0-1.0
    sentence_complexity: float = 0.0  # å¥å­å¤æ‚åº¦ 0.0-1.0

    # å¸¸ç”¨è¯æ±‡
    frequent_words: List[str] = None  # å¸¸ç”¨è¯Top10
    catchphrases: List[str] = None  # å£å¤´ç¦…

    # è¡¨æƒ…å’Œæ ‡ç‚¹ä½¿ç”¨
    emoji_usage_rate: float = 0.0  # è¡¨æƒ…ä½¿ç”¨ç‡
    emoticon_usage_rate: float = 0.0  # é¢œæ–‡å­—ä½¿ç”¨ç‡
    exclamation_usage: float = 0.0  # æ„Ÿå¹å·ä½¿ç”¨ç‡
    question_usage: float = 0.0  # é—®å¥ä½¿ç”¨ç‡

    # æ‰“å­—ä¹ æƒ¯
    avg_typing_speed_estimate: float = 0.0  # ä¼°è®¡çš„æ‰“å­—é€Ÿåº¦ï¼ˆå­—/ç§’ï¼‰
    punctuation_usage: float = 0.0  # æ ‡ç‚¹ç¬¦å·ä½¿ç”¨ç‡

    # è¯­è¨€æ¨¡å¼
    prefers_short_messages: bool = False  # åå¥½çŸ­æ¶ˆæ¯
    uses_internet_slang: bool = False  # ä½¿ç”¨ç½‘ç»œç”¨è¯­
    uses_dialects: bool = False  # ä½¿ç”¨æ–¹è¨€

    # ç‰¹æ®Šæ¨¡å¼
    greeting_style: str = ""  # æ‰“æ‹›å‘¼æ–¹å¼
    farewell_style: str = ""  # å‘Šåˆ«æ–¹å¼

    # æ—¶é—´æˆ³
    timestamp: float = 0.0
    data_points: int = 0

    def __post_init__(self):
        if self.frequent_words is None:
            self.frequent_words = []
        if self.catchphrases is None:
            self.catchphrases = []

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)

    def get_human_readable_summary(self) -> str:
        """è·å–äººç±»å¯è¯»çš„è¯­è¨€é£æ ¼æ‘˜è¦"""
        parts = []

        # é£æ ¼æè¿°
        formality_desc = {"formal": "æ­£å¼", "casual": "éšæ„", "neutral": "ä¸­æ€§"}
        tone_desc = {"serious": "ä¸¥è‚ƒ", "humorous": "å¹½é»˜", "friendly": "å‹å¥½", "neutral": "ä¸­æ€§"}

        style_str = f"{formality_desc.get(self.formality, 'ä¸­æ€§')}ã€{tone_desc.get(self.tone, 'ä¸­æ€§')}"
        parts.append(f"{self.user_nickname or 'ç”¨æˆ·'}è¯´è¯{style_str}")

        # è¡¨æƒ…ä½¿ç”¨
        if self.emoji_usage_rate > 0.3:
            parts.append("ç»å¸¸ä½¿ç”¨è¡¨æƒ…")

        # æ¶ˆæ¯é•¿åº¦
        if self.prefers_short_messages:
            parts.append("å–œæ¬¢ç®€çŸ­è¡¨è¾¾")
        elif self.avg_message_length > 50:
            parts.append("å–œæ¬¢é•¿ç¯‡å¤§è®º")

        # å£å¤´ç¦…
        if self.catchphrases:
            parts.append(f"å£å¤´ç¦…ï¼š{self.catchphrases[0]}")

        return "ï¼Œ".join(parts)


class LanguageStylePerception:
    """è¯­è¨€é£æ ¼æ„ŸçŸ¥å™¨"""

    # ç½‘ç»œç”¨è¯­è¯åº“
    INTERNET_SLANG = {
        "hhh", "å“ˆå“ˆå“ˆ", "hhhh", "233", "666", "awsl", "orz", "yyds",
        "ç»ç»å­", "emo", "ç ´é˜²", "å†…å·", "èººå¹³", "èŠœæ¹–", "æ “Q",
        "u1s1", "xswl", "nsdd", "zqsg", "dbq", "yygq", "awsl",
    }

    # ç¤¼è²Œç”¨è¯­
    POLITE_WORDS = {
        "è¯·", "è°¢è°¢", "éº»çƒ¦", "æ‰“æ‰°", "ä¸å¥½æ„æ€", "æŠ±æ­‰", "å¯¹ä¸èµ·",
        "åŠ³é©¾", "è¾›è‹¦", "æ„Ÿè°¢", "æ‹œæ‰˜", "æ‚¨", "æ•¬è¯·",
    }

    # æ­£å¼ç”¨è¯­
    FORMAL_WORDS = {
        "æ‚¨", "è´µ", "æ•¬", "è¯·", "è°¨", "æ•", "è‡´", "æ•¬è¯·",
        "ä¸èƒœ", "ç”šä¸º", "æ³è¯·", "æ‹œæ‰˜", "å¨æ‰°",
    }

    # å¸¸è§æ‰“æ‹›å‘¼æ–¹å¼
    GREETINGS = [
        "ä½ å¥½", "æ‚¨å¥½", "hi", "hello", "å—¨", "å“ˆå–½", "æ—©", "æ—©ä¸Šå¥½",
        "ä¸­åˆå¥½", "ä¸‹åˆå¥½", "æ™šä¸Šå¥½", "æ™šå®‰", "å¤§å®¶å¥½",
    ]

    # å¸¸è§å‘Šåˆ«æ–¹å¼
    FAREWELLS = [
        "å†è§", "æ‹œæ‹œ", "bye", "88", "886", "æ™šå®‰", "å…ˆèµ°äº†",
        "æºœäº†", "æ’¤äº†", "ä¸‹çº¿äº†", "ç¡äº†",
    ]

    def __init__(self, history_window: int = 30):
        """
        åˆå§‹åŒ–è¯­è¨€é£æ ¼æ„ŸçŸ¥å™¨

        Args:
            history_window: å†å²åˆ†æçª—å£ï¼ˆå¤©ï¼‰
        """
        self.history_window = history_window
        self.user_messages: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        # æ¶ˆæ¯è®°å½•: {"content": str, "timestamp": float, "length": int}

        logger.info(f"è¯­è¨€é£æ ¼æ„ŸçŸ¥æ¨¡å—åˆå§‹åŒ–å®Œæˆï¼Œå†å²çª—å£: {history_window}å¤©")

    def record_message(
        self,
        user_id: str,
        message_content: str,
        timestamp: Optional[float] = None,
    ):
        """
        è®°å½•ç”¨æˆ·æ¶ˆæ¯

        Args:
            user_id: ç”¨æˆ·ID
            message_content: æ¶ˆæ¯å†…å®¹
            timestamp: æ—¶é—´æˆ³
        """
        if timestamp is None:
            timestamp = time.time()

        message_record = {
            "content": message_content,
            "timestamp": timestamp,
            "length": len(message_content),
        }

        self.user_messages[user_id].append(message_record)

        # æ¸…ç†è¿‡æœŸæ•°æ®
        self._cleanup_old_messages(user_id)

    def _cleanup_old_messages(self, user_id: str):
        """æ¸…ç†è¿‡æœŸæ¶ˆæ¯"""
        cutoff_time = time.time() - (self.history_window * 86400)
        self.user_messages[user_id] = [
            msg for msg in self.user_messages[user_id]
            if msg["timestamp"] >= cutoff_time
        ]

    def _analyze_formality(self, messages: List[str]) -> str:
        """åˆ†ææ­£å¼ç¨‹åº¦"""
        formal_count = 0
        casual_count = 0

        for msg in messages:
            # æ­£å¼ç”¨è¯­
            formal_count += sum(1 for word in self.FORMAL_WORDS if word in msg)

            # éšæ„è¡¨è¾¾ï¼ˆè¡¨æƒ…ã€è¯­æ°”è¯ã€ç½‘ç»œç”¨è¯­ï¼‰
            has_emoji = bool(re.search(r'[ğŸ˜€-ğŸ™]', msg))
            has_emoticon = bool(re.search(r'[ï¼ˆ(][^)]*[ï¼‰)]|[><]|[oO][_-][oO]', msg))
            has_slang = any(slang in msg for slang in self.INTERNET_SLANG)

            if has_emoji or has_emoticon or has_slang:
                casual_count += 1

        if formal_count > casual_count * 2:
            return "formal"
        elif casual_count > formal_count * 2:
            return "casual"
        else:
            return "neutral"

    def _analyze_tone(self, messages: List[str]) -> str:
        """åˆ†æè¯­æ°”"""
        humor_indicators = ["å“ˆå“ˆ", "hh", "ç¬‘", "ğŸ˜‚", "ğŸ¤£", "ğŸ˜„", "æœ‰è¶£", "å¥½ç©"]
        serious_indicators = ["é‡è¦", "ä¸¥è‚ƒ", "è®¤çœŸ", "å¿…é¡»", "åŠ¡å¿…"]
        friendly_indicators = ["å—¯", "å“¦", "å‘€", "å‘¢", "å§", "å˜›", "å“Ÿ", "ğŸ˜Š", "ğŸ˜"]

        humor_score = sum(
            1 for msg in messages
            for indicator in humor_indicators
            if indicator in msg
        )
        serious_score = sum(
            1 for msg in messages
            for indicator in serious_indicators
            if indicator in msg
        )
        friendly_score = sum(
            1 for msg in messages
            for indicator in friendly_indicators
            if indicator in msg
        )

        scores = {
            "humorous": humor_score,
            "serious": serious_score,
            "friendly": friendly_score,
        }

        if max(scores.values()) == 0:
            return "neutral"

        return max(scores, key=scores.get)

    def _analyze_politeness(self, messages: List[str]) -> str:
        """åˆ†æç¤¼è²Œç¨‹åº¦"""
        polite_count = sum(
            1 for msg in messages
            for word in self.POLITE_WORDS
            if word in msg
        )

        avg_polite = polite_count / len(messages) if messages else 0

        if avg_polite > 0.3:
            return "polite"
        elif avg_polite < 0.05:
            return "casual"
        else:
            return "neutral"

    def _extract_vocabulary(self, messages: List[str]) -> tuple[List[str], float]:
        """
        æå–å¸¸ç”¨è¯æ±‡

        Returns:
            (frequent_words, vocabulary_richness)
        """
        all_words = []
        for msg in messages:
            # æå–ä¸­æ–‡è¯ï¼ˆ2-4å­—ï¼‰
            chinese_words = re.findall(r'[\u4e00-\u9fa5]{2,4}', msg)
            all_words.extend(chinese_words)

        if not all_words:
            return [], 0.0

        # åœç”¨è¯
        stopwords = {"çš„", "äº†", "æ˜¯", "åœ¨", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ", "ä»¬", "è¿™", "é‚£", "å’Œ", "ä¸"}
        all_words = [w for w in all_words if w not in stopwords]

        # è¯é¢‘ç»Ÿè®¡
        word_freq = Counter(all_words)
        frequent_words = [word for word, count in word_freq.most_common(10)]

        # è¯æ±‡ä¸°å¯Œåº¦
        vocabulary_richness = len(set(all_words)) / len(all_words) if all_words else 0.0

        return frequent_words, vocabulary_richness

    def _detect_catchphrases(self, messages: List[str]) -> List[str]:
        """æ£€æµ‹å£å¤´ç¦…"""
        # ç®€å•æ£€æµ‹ï¼šé«˜é¢‘çŸ­è¯­
        phrases = []
        for msg in messages:
            # æå–2-5å­—çš„çŸ­è¯­
            chinese_phrases = re.findall(r'[\u4e00-\u9fa5]{2,5}', msg)
            phrases.extend(chinese_phrases)

        phrase_freq = Counter(phrases)
        # è‡³å°‘å‡ºç°3æ¬¡æ‰ç®—å£å¤´ç¦…
        catchphrases = [
            phrase for phrase, count in phrase_freq.most_common(5)
            if count >= 3
        ]

        return catchphrases

    def _calculate_emoji_usage(self, messages: List[str]) -> tuple[float, float]:
        """
        è®¡ç®—è¡¨æƒ…ä½¿ç”¨ç‡

        Returns:
            (emoji_rate, emoticon_rate)
        """
        emoji_count = sum(1 for msg in messages if re.search(r'[ğŸ˜€-ğŸ™]', msg))
        emoticon_count = sum(
            1 for msg in messages
            if re.search(r'[ï¼ˆ(][^)]*[ï¼‰)]|[><]|[oO][_-][oO]|qwq|owo|uwu', msg, re.IGNORECASE)
        )

        emoji_rate = emoji_count / len(messages) if messages else 0.0
        emoticon_rate = emoticon_count / len(messages) if messages else 0.0

        return emoji_rate, emoticon_rate

    def _calculate_punctuation_usage(self, messages: List[str]) -> tuple[float, float]:
        """
        è®¡ç®—æ ‡ç‚¹ä½¿ç”¨ç‡

        Returns:
            (exclamation_rate, question_rate)
        """
        exclamation_count = sum(1 for msg in messages if '!' in msg or 'ï¼' in msg)
        question_count = sum(1 for msg in messages if '?' in msg or 'ï¼Ÿ' in msg)

        exclamation_rate = exclamation_count / len(messages) if messages else 0.0
        question_rate = question_count / len(messages) if messages else 0.0

        return exclamation_rate, question_rate

    def _detect_greeting_style(self, messages: List[str]) -> str:
        """æ£€æµ‹æ‰“æ‹›å‘¼æ–¹å¼"""
        for msg in messages:
            msg_lower = msg.lower()
            for greeting in self.GREETINGS:
                if greeting in msg_lower:
                    return greeting
        return ""

    def _detect_farewell_style(self, messages: List[str]) -> str:
        """æ£€æµ‹å‘Šåˆ«æ–¹å¼"""
        # æ£€æŸ¥æœ€åå‡ æ¡æ¶ˆæ¯
        recent_messages = messages[-10:]
        for msg in recent_messages:
            msg_lower = msg.lower()
            for farewell in self.FAREWELLS:
                if farewell in msg_lower:
                    return farewell
        return ""

    def get_language_style(self, user_id: str, user_nickname: str = "") -> LanguageStyle:
        """
        è·å–ç”¨æˆ·è¯­è¨€é£æ ¼

        Args:
            user_id: ç”¨æˆ·ID
            user_nickname: ç”¨æˆ·æ˜µç§°

        Returns:
            LanguageStyleå¯¹è±¡
        """
        message_records = self.user_messages.get(user_id, [])

        if not message_records:
            return LanguageStyle(
                user_id=user_id,
                user_nickname=user_nickname,
                timestamp=time.time(),
                data_points=0,
            )

        messages = [record["content"] for record in message_records]
        message_lengths = [record["length"] for record in message_records]

        # åˆ†æé£æ ¼
        formality = self._analyze_formality(messages)
        tone = self._analyze_tone(messages)
        politeness = self._analyze_politeness(messages)

        # è¯­è¨€ç‰¹å¾
        avg_length = sum(message_lengths) / len(message_lengths)
        frequent_words, vocab_richness = self._extract_vocabulary(messages)
        catchphrases = self._detect_catchphrases(messages)

        # å¥å­å¤æ‚åº¦ï¼ˆç®€åŒ–ï¼šåŸºäºå¹³å‡é•¿åº¦å’Œæ ‡ç‚¹æ•°é‡ï¼‰
        avg_punctuation = sum(msg.count('ï¼Œ') + msg.count('ã€‚') + msg.count(',') + msg.count('.') for msg in messages) / len(messages)
        sentence_complexity = min(1.0, (avg_length / 50 + avg_punctuation / 3) / 2)

        # è¡¨æƒ…å’Œæ ‡ç‚¹
        emoji_rate, emoticon_rate = self._calculate_emoji_usage(messages)
        exclamation_rate, question_rate = self._calculate_punctuation_usage(messages)

        # æ‰“å­—é€Ÿåº¦ä¼°è®¡ï¼ˆå¦‚æœæœ‰æ—¶é—´æˆ³å¯ä»¥è®¡ç®—ï¼‰
        typing_speed = 0.0  # TODO: éœ€è¦æ›´ç²¾ç¡®çš„æ—¶é—´æˆ³

        # æ ‡ç‚¹ä½¿ç”¨ç‡
        punct_chars = sum(
            msg.count('ï¼Œ') + msg.count('ã€‚') + msg.count('ï¼') + msg.count('ï¼Ÿ') +
            msg.count(',') + msg.count('.') + msg.count('!') + msg.count('?')
            for msg in messages
        )
        total_chars = sum(len(msg) for msg in messages)
        punctuation_usage = punct_chars / total_chars if total_chars > 0 else 0.0

        # è¯­è¨€æ¨¡å¼
        prefers_short = avg_length < 15
        uses_slang = any(slang in msg for msg in messages for slang in self.INTERNET_SLANG)

        # æ£€æµ‹æ–¹è¨€ï¼ˆç®€åŒ–ï¼šæ£€æµ‹ç‰¹å®šæ–¹è¨€è¯æ±‡ï¼‰
        dialect_words = {"å˜", "å’§", "å˜›", "æ’’", "å“ˆ", "å—¦", "å˜", "å“¦è±"}
        uses_dialects = sum(1 for msg in messages for word in dialect_words if word in msg) > len(messages) * 0.1

        # æ‰“æ‹›å‘¼å’Œå‘Šåˆ«æ–¹å¼
        greeting_style = self._detect_greeting_style(messages)
        farewell_style = self._detect_farewell_style(messages)

        return LanguageStyle(
            user_id=user_id,
            user_nickname=user_nickname,
            formality=formality,
            tone=tone,
            politeness=politeness,
            avg_message_length=avg_length,
            vocabulary_richness=vocab_richness,
            sentence_complexity=sentence_complexity,
            frequent_words=frequent_words,
            catchphrases=catchphrases,
            emoji_usage_rate=emoji_rate,
            emoticon_usage_rate=emoticon_rate,
            exclamation_usage=exclamation_rate,
            question_usage=question_rate,
            avg_typing_speed_estimate=typing_speed,
            punctuation_usage=punctuation_usage,
            prefers_short_messages=prefers_short,
            uses_internet_slang=uses_slang,
            uses_dialects=uses_dialects,
            greeting_style=greeting_style,
            farewell_style=farewell_style,
            timestamp=time.time(),
            data_points=len(message_records),
        )
