"""
å®‰å…¨æ„ŸçŸ¥æ¨¡å—
æ£€æµ‹æ•æ„Ÿå†…å®¹ã€å¼‚å¸¸è¡Œä¸ºã€åƒåœ¾ä¿¡æ¯ã€é£é™©è¯„ä¼°ç­‰
"""

import time
import re
from typing import Dict, Optional, Any, List
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
from src.common.logger import get_logger

logger = get_logger("security_perception")


@dataclass
class SecurityStatus:
    """å®‰å…¨çŠ¶æ€æ•°æ®ç±»"""

    chat_id: str = ""
    user_id: str = ""

    # é£é™©è¯„ä¼°
    risk_level: str = "safe"  # "safe" | "low" | "medium" | "high" | "critical"
    risk_score: float = 0.0  # é£é™©åˆ†æ•° 0.0-100.0

    # æ£€æµ‹ç»“æœ
    has_sensitive_content: bool = False
    has_spam: bool = False
    has_malicious_link: bool = False
    has_abnormal_behavior: bool = False

    # è¯¦ç»†ä¿¡æ¯
    detected_issues: List[str] = None  # æ£€æµ‹åˆ°çš„é—®é¢˜åˆ—è¡¨
    sensitive_keywords: List[str] = None  # è§¦å‘çš„æ•æ„Ÿè¯
    spam_indicators: List[str] = None  # åƒåœ¾ä¿¡æ¯æŒ‡æ ‡

    # ç”¨æˆ·è¡Œä¸ºå¼‚å¸¸
    abnormal_patterns: List[str] = None  # å¼‚å¸¸æ¨¡å¼
    suspicious_activity: bool = False

    # æ—¶é—´æˆ³
    timestamp: float = 0.0

    def __post_init__(self):
        if self.detected_issues is None:
            self.detected_issues = []
        if self.sensitive_keywords is None:
            self.sensitive_keywords = []
        if self.spam_indicators is None:
            self.spam_indicators = []
        if self.abnormal_patterns is None:
            self.abnormal_patterns = []

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)

    def get_human_readable_summary(self) -> str:
        """è·å–äººç±»å¯è¯»çš„å®‰å…¨çŠ¶æ€æ‘˜è¦"""
        if self.risk_level == "safe":
            return "å®‰å…¨çŠ¶æ€è‰¯å¥½"

        parts = []
        risk_desc = {
            "low": "ä½é£é™©",
            "medium": "ä¸­ç­‰é£é™©",
            "high": "é«˜é£é™©",
            "critical": "ä¸¥é‡é£é™©",
        }

        parts.append(f"å®‰å…¨ç­‰çº§ï¼š{risk_desc.get(self.risk_level, 'æœªçŸ¥')}")

        if self.detected_issues:
            parts.append(f"æ£€æµ‹åˆ°{len(self.detected_issues)}ä¸ªé—®é¢˜")

        return "ï¼Œ".join(parts)


class SecurityPerception:
    """å®‰å…¨æ„ŸçŸ¥å™¨"""

    # æ•æ„Ÿè¯åº“ï¼ˆç¤ºä¾‹ï¼Œå®é™…åº”è¯¥æ›´å®Œå–„ï¼‰
    SENSITIVE_KEYWORDS = {
        # æ”¿æ²»æ•æ„Ÿ
        "æ”¿æ²»æ•æ„Ÿè¯1", "æ”¿æ²»æ•æ„Ÿè¯2",  # å®é™…åº”è¯¥é…ç½®çœŸå®çš„æ•æ„Ÿè¯

        # è‰²æƒ…æš´åŠ›
        "è‰²æƒ…è¯1", "æš´åŠ›è¯1",

        # è¯ˆéª—ç›¸å…³
        "è½¬è´¦", "æ±‡æ¬¾", "ä¸­å¥–", "å…è´¹é¢†å–", "ç‚¹å‡»é¢†å¥–",
    }

    # åƒåœ¾ä¿¡æ¯ç‰¹å¾
    SPAM_PATTERNS = [
        r'(åŠ |æ·»åŠ ).{0,5}(å¾®ä¿¡|QQ|vx)',  # æ·»åŠ è”ç³»æ–¹å¼
        r'(å…è´¹|é™æ—¶).{0,10}(é¢†å–|è·å¾—)',  # å…è´¹é¢†å–
        r'(ç‚¹å‡»|å¤åˆ¶).{0,10}(é“¾æ¥|ç½‘å€)',  # ç‚¹å‡»é“¾æ¥
        r'[0-9]{6,}',  # é•¿æ•°å­—ä¸²
    ]

    # å¯ç–‘é“¾æ¥æ¨¡å¼
    SUSPICIOUS_URL_PATTERNS = [
        r'bit\.ly',
        r't\.cn',
        r'çŸ­ç½‘å€',
    ]

    # æ¬ºè¯ˆæŒ‡æ ‡
    FRAUD_KEYWORDS = {
        "ä¸­å¥–", "å…è´¹", "æ­å–œæ‚¨", "è½¬è´¦", "æ±‡æ¬¾", "å¯†ç ",
        "éªŒè¯ç ", "é“¶è¡Œå¡", "èº«ä»½è¯", "ç´§æ€¥", "ç«‹å³",
    }

    def __init__(self, sensitivity: str = "medium"):
        """
        åˆå§‹åŒ–å®‰å…¨æ„ŸçŸ¥å™¨

        Args:
            sensitivity: æ•æ„Ÿåº¦ "low" | "medium" | "high"
        """
        self.sensitivity = sensitivity

        # ç”¨æˆ·è¡Œä¸ºå†å²ï¼ˆç”¨äºå¼‚å¸¸æ£€æµ‹ï¼‰
        # {user_id: {"message_times": [], "message_contents": []}}
        self.user_history: Dict[str, Dict[str, List]] = defaultdict(
            lambda: {"message_times": [], "message_contents": []}
        )

        # æ•æ„Ÿåº¦é˜ˆå€¼
        self.thresholds = {
            "low": {"spam_score": 80, "fraud_score": 90},
            "medium": {"spam_score": 60, "fraud_score": 70},
            "high": {"spam_score": 40, "fraud_score": 50},
        }

        logger.info(f"å®‰å…¨æ„ŸçŸ¥æ¨¡å—åˆå§‹åŒ–å®Œæˆï¼Œæ•æ„Ÿåº¦: {sensitivity}")

    def analyze_message(
        self,
        chat_id: str,
        user_id: str,
        message_content: str,
        timestamp: Optional[float] = None,
    ) -> SecurityStatus:
        """
        åˆ†ææ¶ˆæ¯å®‰å…¨æ€§

        Args:
            chat_id: èŠå¤©ID
            user_id: ç”¨æˆ·ID
            message_content: æ¶ˆæ¯å†…å®¹
            timestamp: æ—¶é—´æˆ³

        Returns:
            SecurityStatuså¯¹è±¡
        """
        if timestamp is None:
            timestamp = time.time()

        # è®°å½•å†å²
        self.user_history[user_id]["message_times"].append(timestamp)
        self.user_history[user_id]["message_contents"].append(message_content)

        # æ¸…ç†æ—§æ•°æ®ï¼ˆä¿ç•™7å¤©ï¼‰
        cutoff = timestamp - (7 * 86400)
        history = self.user_history[user_id]
        valid_indices = [i for i, t in enumerate(history["message_times"]) if t >= cutoff]
        history["message_times"] = [history["message_times"][i] for i in valid_indices]
        history["message_contents"] = [history["message_contents"][i] for i in valid_indices]

        # æ‰§è¡Œå„é¡¹æ£€æµ‹
        detected_issues = []
        sensitive_keywords = []
        spam_indicators = []
        abnormal_patterns = []

        # 1. æ•æ„Ÿå†…å®¹æ£€æµ‹
        has_sensitive, keywords = self._detect_sensitive_content(message_content)
        if has_sensitive:
            detected_issues.append("åŒ…å«æ•æ„Ÿå†…å®¹")
            sensitive_keywords = keywords

        # 2. åƒåœ¾ä¿¡æ¯æ£€æµ‹
        is_spam, indicators = self._detect_spam(message_content)
        if is_spam:
            detected_issues.append("ç–‘ä¼¼åƒåœ¾ä¿¡æ¯")
            spam_indicators = indicators

        # 3. æ¶æ„é“¾æ¥æ£€æµ‹
        has_malicious_link = self._detect_malicious_links(message_content)
        if has_malicious_link:
            detected_issues.append("åŒ…å«å¯ç–‘é“¾æ¥")

        # 4. å¼‚å¸¸è¡Œä¸ºæ£€æµ‹
        has_abnormal, patterns = self._detect_abnormal_behavior(user_id, message_content, timestamp)
        if has_abnormal:
            detected_issues.append("æ£€æµ‹åˆ°å¼‚å¸¸è¡Œä¸º")
            abnormal_patterns = patterns

        # 5. æ¬ºè¯ˆæ£€æµ‹
        fraud_score = self._detect_fraud(message_content)
        if fraud_score > self.thresholds[self.sensitivity]["fraud_score"]:
            detected_issues.append("ç–‘ä¼¼è¯ˆéª—ä¿¡æ¯")

        # è®¡ç®—é£é™©åˆ†æ•°å’Œç­‰çº§
        risk_score = self._calculate_risk_score(
            has_sensitive, is_spam, has_malicious_link, has_abnormal, fraud_score
        )
        risk_level = self._determine_risk_level(risk_score)

        return SecurityStatus(
            chat_id=chat_id,
            user_id=user_id,
            risk_level=risk_level,
            risk_score=risk_score,
            has_sensitive_content=has_sensitive,
            has_spam=is_spam,
            has_malicious_link=has_malicious_link,
            has_abnormal_behavior=has_abnormal,
            detected_issues=detected_issues,
            sensitive_keywords=sensitive_keywords,
            spam_indicators=spam_indicators,
            abnormal_patterns=abnormal_patterns,
            suspicious_activity=len(detected_issues) > 0,
            timestamp=timestamp,
        )

    def _detect_sensitive_content(self, message: str) -> tuple[bool, List[str]]:
        """æ£€æµ‹æ•æ„Ÿå†…å®¹"""
        found_keywords = []

        for keyword in self.SENSITIVE_KEYWORDS:
            if keyword in message:
                found_keywords.append(keyword)

        return len(found_keywords) > 0, found_keywords

    def _detect_spam(self, message: str) -> tuple[bool, List[str]]:
        """æ£€æµ‹åƒåœ¾ä¿¡æ¯"""
        indicators = []

        # æ£€æµ‹æ¨¡å¼
        for pattern in self.SPAM_PATTERNS:
            if re.search(pattern, message):
                indicators.append(f"åŒ¹é…æ¨¡å¼: {pattern[:20]}")

        # æ£€æµ‹é‡å¤å­—ç¬¦
        if re.search(r'(.)\1{5,}', message):
            indicators.append("å¤§é‡é‡å¤å­—ç¬¦")

        # æ£€æµ‹å…¨å¤§å†™
        if len(message) > 10 and message.isupper():
            indicators.append("å…¨å¤§å†™æ–‡æœ¬")

        # æ£€æµ‹è¿‡å¤šè¡¨æƒ…
        emoji_count = len(re.findall(r'[ğŸ˜€-ğŸ™]', message))
        if emoji_count > 10:
            indicators.append("è¿‡å¤šè¡¨æƒ…ç¬¦å·")

        spam_score = len(indicators) * 25  # æ¯ä¸ªæŒ‡æ ‡25åˆ†

        return spam_score > self.thresholds[self.sensitivity]["spam_score"], indicators

    def _detect_malicious_links(self, message: str) -> bool:
        """æ£€æµ‹æ¶æ„é“¾æ¥"""
        # æ£€æµ‹URL
        urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', message)

        for url in urls:
            for pattern in self.SUSPICIOUS_URL_PATTERNS:
                if re.search(pattern, url):
                    return True

        return False

    def _detect_abnormal_behavior(
        self, user_id: str, message: str, timestamp: float
    ) -> tuple[bool, List[str]]:
        """æ£€æµ‹å¼‚å¸¸è¡Œä¸º"""
        abnormal_patterns = []
        history = self.user_history[user_id]

        # 1. çŸ­æ—¶é—´å†…å¤§é‡å‘é€
        recent_times = [t for t in history["message_times"] if timestamp - t < 60]
        if len(recent_times) > 10:
            abnormal_patterns.append("çŸ­æ—¶é—´å†…é¢‘ç¹å‘é€æ¶ˆæ¯")

        # 2. é‡å¤å†…å®¹
        recent_contents = history["message_contents"][-10:]
        if message in recent_contents[:-1]:  # æ’é™¤å½“å‰æ¶ˆæ¯
            duplicate_count = recent_contents.count(message)
            if duplicate_count > 2:
                abnormal_patterns.append("å‘é€é‡å¤å†…å®¹")

        # 3. æ¶ˆæ¯é•¿åº¦å¼‚å¸¸
        if len(message) > 1000:
            abnormal_patterns.append("æ¶ˆæ¯é•¿åº¦å¼‚å¸¸")

        # 4. çªç„¶æ”¹å˜å‘è¨€æ¨¡å¼ï¼ˆä»ä¸å‘è¨€åˆ°å¤§é‡å‘è¨€ï¼‰
        if len(recent_times) > 5 and len(history["message_times"]) > 10:
            older_activity = len([t for t in history["message_times"] if timestamp - t > 3600])
            if older_activity < 5:  # ä¹‹å‰ä¸æ´»è·ƒ
                abnormal_patterns.append("å‘è¨€æ¨¡å¼çªå˜")

        return len(abnormal_patterns) > 0, abnormal_patterns

    def _detect_fraud(self, message: str) -> float:
        """æ£€æµ‹æ¬ºè¯ˆï¼ˆè¿”å›åˆ†æ•°0-100ï¼‰"""
        fraud_score = 0.0

        # æ£€æµ‹æ¬ºè¯ˆå…³é”®è¯
        for keyword in self.FRAUD_KEYWORDS:
            if keyword in message:
                fraud_score += 15

        # æ£€æµ‹é‡‘é¢ç›¸å…³
        if re.search(r'[0-9,]+å…ƒ|ï¿¥[0-9,]+|[0-9]+å—é’±', message):
            fraud_score += 20

        # æ£€æµ‹ç´§æ€¥æ€§ç”¨è¯
        urgent_words = ["é©¬ä¸Š", "ç«‹å³", "èµ¶å¿«", "é™æ—¶", "ç´§æ€¥"]
        if any(word in message for word in urgent_words):
            fraud_score += 10

        return min(100.0, fraud_score)

    def _calculate_risk_score(
        self,
        has_sensitive: bool,
        is_spam: bool,
        has_malicious_link: bool,
        has_abnormal: bool,
        fraud_score: float,
    ) -> float:
        """è®¡ç®—ç»¼åˆé£é™©åˆ†æ•°"""
        score = 0.0

        if has_sensitive:
            score += 40
        if is_spam:
            score += 30
        if has_malicious_link:
            score += 25
        if has_abnormal:
            score += 20

        score += fraud_score * 0.5

        return min(100.0, score)

    def _determine_risk_level(self, risk_score: float) -> str:
        """åˆ¤å®šé£é™©ç­‰çº§"""
        if risk_score >= 80:
            return "critical"
        elif risk_score >= 60:
            return "high"
        elif risk_score >= 40:
            return "medium"
        elif risk_score >= 20:
            return "low"
        else:
            return "safe"

    def get_user_security_summary(self, user_id: str) -> Dict[str, Any]:
        """
        è·å–ç”¨æˆ·å®‰å…¨æ‘˜è¦

        Args:
            user_id: ç”¨æˆ·ID

        Returns:
            å®‰å…¨æ‘˜è¦å­—å…¸
        """
        history = self.user_history.get(user_id, {"message_times": [], "message_contents": []})

        total_messages = len(history["message_contents"])
        if total_messages == 0:
            return {
                "user_id": user_id,
                "total_messages": 0,
                "risk_level": "safe",
                "is_trustworthy": True,
            }

        # åˆ†ææœ€è¿‘æ¶ˆæ¯çš„é£é™©
        recent_messages = history["message_contents"][-20:]
        risk_count = 0

        for msg in recent_messages:
            status = self.analyze_message("", user_id, msg)
            if status.risk_level not in ["safe", "low"]:
                risk_count += 1

        risk_ratio = risk_count / len(recent_messages)

        return {
            "user_id": user_id,
            "total_messages": total_messages,
            "risk_messages": risk_count,
            "risk_ratio": risk_ratio,
            "risk_level": "high" if risk_ratio > 0.3 else "medium" if risk_ratio > 0.1 else "safe",
            "is_trustworthy": risk_ratio < 0.1,
        }
